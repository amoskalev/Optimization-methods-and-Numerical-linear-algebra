{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moskalev Artem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 (LU decomposition)\n",
    "## 30 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LU for band matrices (5 pts)\n",
    "\n",
    "The complexity to find an LU decomposition of a dense $n\\times n$ matrix is $\\mathcal{O}(n^3)$.\n",
    "Significant reduction in complexity can be achieved if the matrix has a certain structure, e.g. it is sparse. \n",
    "In the following task we consider an important example of $LU$ for a special type of sparse matrices –– tridiagonal matrices.\n",
    "\n",
    "- Find the number of operations to compute an $LU$ decomposition of a tridiagonal matrix taking into account only non-zero elements. How many nonzero elements are in factors $L$, $U$ and where are they located? Conclude what is the complexity to solve a linear system with tridiagonal matrix in terms of $n$. \n",
    "\n",
    "### 2. Completing the proof of existence of LU (10 pts)\n",
    "\n",
    "Some details in lecture proofs about $LU$ were omitted. Let us complete them.\n",
    "- Prove that if $LU$ decomposition exists, then matrix is strictly regular.\n",
    "- Prove that if $A$ is strictly regular, then $A_1 = D - \\frac 1a b c^T$ (see lectures for notations) is also strictly regular.\n",
    "\n",
    "### 3. Stability of LU (10 pts)\n",
    "\n",
    "Let\n",
    "$A = \\begin{pmatrix}\n",
    "a & 1 & 0\\\\\n",
    "1 & 1 & 1 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{pmatrix}.$ \n",
    "* Find analytically an $LU$ decomposition of the matrix $A$.\n",
    "* For what values of $a$ does the LU decomposition of $A$ exist?\n",
    "* Explain, why can the LU decomposition fail to approximate factors $L$ and $U$ for $|a|\\ll 1$ in computer arithmetic?\n",
    "How can this problem be solved?\n",
    "\n",
    "\n",
    "### 4. Block LU (5 pts)\n",
    "\n",
    "Let $A = \\begin{bmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{bmatrix}$ be a block matrix. The goal is to solve the linear system\n",
    "$$\n",
    "     \\begin{bmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{bmatrix} \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix} = \\begin{bmatrix} f_1 \\\\ f_2 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "* Using block elimination find matrix $S$ and right-hand side $f_2$ so that $u_2$ can be found from $S u_2 = f_2$. Note that the matrix $S$ is called <font color='red'> Schur complement </font> of the block $A_{11}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $L$ and $U$ matrices will both be bidiagonal and will consist of $2n-1$ elements each.<br>\n",
    "<br>\n",
    "- An order for tridiagonal matrix LU decomposition will be $\\mathcal{O}(n)$. Number of operations required is $3(n-1)+1$<br>\n",
    "<br>\n",
    "- In terms of number of operations, while solwing system with $LU$ we have to perform $3(n−1)$ additions, $3(n−1)$ multiplications, and $2n$ divisions. Thus, in total we have $8n−6$ operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $LU$ exists, that means that matrices $L$ and $U$ are non-singular at least (They have no zero elements on diagonal by definition).<br>\n",
    "Matrix $A$ is called stricktly regular if all of its principal minors are non-singular. Meanwhile, each principle minor is obtained from corresponding minors from U and L."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $A = \\begin{pmatrix}\n",
    "a & 1 & 0\\\\\n",
    "1 & 1 & 1 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{pmatrix}$\n",
    "<br>\n",
    "<br>\n",
    "$\\begin{pmatrix}\n",
    "1 & 0 & 0\\\\\n",
    "-\\frac1a & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}_{E_{21}}$\n",
    "$\\begin{pmatrix}\n",
    "a & 1 & 0\\\\\n",
    "1 & 1 & 1 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{pmatrix}=$\n",
    "$\\begin{pmatrix}\n",
    "a & 1 & 0\\\\\n",
    "0 & 1-\\frac1a & 1 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{pmatrix}$ if $a\\neq0$\n",
    "<br>\n",
    "<br>\n",
    "$\\begin{pmatrix}\n",
    "1 & 0 & 0\\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}_{E_{31}}$\n",
    "$\\begin{pmatrix}\n",
    "a & 1 & 0\\\\\n",
    "0 & 1 & 1 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{pmatrix}=$\n",
    "$\\begin{pmatrix}\n",
    "a & 1 & 0\\\\\n",
    "0 & 1-\\frac1a & 1 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{pmatrix}$\n",
    "<br>\n",
    "<br>\n",
    "$\\begin{pmatrix}\n",
    "1 & 0 & 0\\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 &-\\frac{a}{a-1} & 1\n",
    "\\end{pmatrix}_{E_{32}}$\n",
    "$\\begin{pmatrix}\n",
    "a & 1 & 0\\\\\n",
    "0 & 1 & 1 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{pmatrix}=$\n",
    "$\\begin{pmatrix}\n",
    "a & 1 & 0\\\\\n",
    "0 & 1-\\frac1a & 1 \\\\\n",
    "0 & 0 & -\\frac{a}{a-1}\n",
    "\\end{pmatrix}_U$ if $a\\neq1$<br>\n",
    "<br>\n",
    "<br>\n",
    "$\\begin{pmatrix}\n",
    "1 & 0 & 0\\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 &-\\frac{a}{a-1} & 1\n",
    "\\end{pmatrix}_{E_{32}}$\n",
    "$\\begin{pmatrix}\n",
    "1 & 0 & 0\\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}_{E_{31}}$\n",
    "$\\begin{pmatrix}\n",
    "1 & 0 & 0\\\\\n",
    "-\\frac1a & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}_{E_{21}}$\n",
    "$\\begin{pmatrix}\n",
    "a & 1 & 0\\\\\n",
    "1 & 1 & 1 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{pmatrix}_A=$\n",
    "$\\begin{pmatrix}\n",
    "a & 1 & 0\\\\\n",
    "0 & 1-\\frac1a & 1 \\\\\n",
    "0 & 0 & -\\frac{a}{a-1}\n",
    "\\end{pmatrix}_U$<br>\n",
    "<br>\n",
    "Thus, we have:\n",
    "<br>\n",
    "<br>\n",
    "$(1)$ $E_{32}E_{31}E_{21}A=U,$ I suggest to omit matrix $E_{31}$ because it is just an identity matrix, so we can rewrite $(1)$ as:\n",
    "<br>\n",
    "<br>\n",
    "$A=E_{21}^{-1}E_{32}^{-1}U$\n",
    "<br>\n",
    "<br>\n",
    "$A=\\begin{pmatrix}\n",
    "1 & 0 & 0\\\\\n",
    "\\frac{1}{a} & 1 & 0 \\\\\n",
    "0 & \\frac{a}{a-1} & 1\n",
    "\\end{pmatrix}_L$\n",
    "$\\begin{pmatrix}\n",
    "a & 1 & 0\\\\\n",
    "0 & 1-\\frac1a & 1 \\\\\n",
    "0 & 0 & -\\frac1{a-1}\n",
    "\\end{pmatrix}_U=$\n",
    "$\\begin{pmatrix}\n",
    "a & 1 & 0\\\\\n",
    "1 & 1 & 1 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{pmatrix}$<br>\n",
    "<br>\n",
    "- LU exist when $a\\neq0, a\\neq1$<br>\n",
    "<br>\n",
    "- LU fails to approximate L and U if $|a|\\ll 1$. Indeed, the presence of small pivots leads to instability. When computations are performed in floating point arithmetic, the number is not represented exactly but will be rounded to the nearest floating point. That causes error. Gradually, such error is accumulated heavily and makes LU decomposition to fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Block LU (5 pts)\n",
    "\n",
    "Let $A = \\begin{bmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{bmatrix}$ be a block matrix. The goal is to solve the linear system\n",
    "$$\n",
    "     \\begin{bmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{bmatrix} \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix} = \\begin{bmatrix} f_1 \\\\ f_2 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "* Using block elimination find matrix $S$ and right-hand side $f_2$ so that $u_2$ can be found from $S u_2 = f_2$. Note that the matrix $S$ is called <font color='red'> Schur complement </font> of the block $A_{11}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{pmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{pmatrix}$$\n",
    "<center> Let's perform elimination process:\n",
    "$$\\begin{pmatrix} A_{11} & A_{12} \\\\ A_{21} - A_{21}A_{11}^{-1}A_{11} & A_{22}- A_{21}A_{11}^{-1}A_{12} \\end{pmatrix}$$ = \n",
    "$$\\begin{pmatrix} A_{11} & A_{12} \\\\ O & A_{22}- A_{21}A_{11}^{-1}A_{12} \\end{pmatrix}$$\n",
    "\n",
    "<center>Thus,$S = A_{22}- A_{21}A_{11}^{-1}A_{12}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 (QR decomposition) \n",
    "\n",
    "## 20 pts\n",
    "\n",
    "### 1. Standard Gram-Schmidt algorithm (10 pts)\n",
    "Our goal now is to orthogonalize a system of linearly independent vectors $v_1,\\dots,v_n$.\n",
    "The standard algorithm for the task is the Gram-Schmidt algorithm:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "u_1 &= v_1, \\\\\n",
    "u_2 &= v_2 - \\frac{(v_2, u_1)}{(u_1, u_1)} u_1, \\\\\n",
    "\\dots \\\\\n",
    "u_n &= v_n - \\frac{(v_n, u_1)}{(u_1, u_1)} u_1 - \\frac{(v_n, u_2)}{(u_2, u_2)} u_2 - \\dots - \\frac{(v_n, u_{n-1})}{(u_{n-1}, u_{n-1})} u_{n-1}.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Now $u_1, \\dots, u_n$ are orthogonal vectors in exact arithmetics. Then to get orthonormal system you should divide each of the vectors by its norm: $u_i := u_i/\\|u_i\\|$.\n",
    "The Gram-Schidt process can be viewed as a QR decomposition. Let us show that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write out what is $Q$ and $R$ obtained in the process described. \n",
    "\n",
    "\n",
    "* Implement the described Gram-Schmidt algorithm as a function ```gram_schmidt(A)```, which outputs ```Q,R``` and check it on a random $100\\times 100$ matrix $B.$ Print out the error. \n",
    "\n",
    "**Note:** To check orthogonality calculate the matrix of scalar products $G_{ij} = (u_i, u_j)$ (called <font color='red'> Gram matrix </font> of set of vectors $u_1,\\dots, u_n$) which should be equal to the identity matrix $I.$ Error $\\|G - I\\|_2$ will show you how far is the system $u_i$ from orthonormal.\n",
    "\n",
    "\n",
    "* Create a Hilbert matrix $A$ of size $100\\times 100$ without using loops.\n",
    "Othogonalize its columns by the described Gram-Schmidt algorithm. Is the Gram matrix close to the identity matrix in this case? Why?\n",
    "\n",
    "\n",
    "The observed loss of orthogonality is a problem of this particular algorithm. To avoid it [modified Gram-Schmidt algorithm](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process#Numerical_stability), QR via Householder reflections or Givens rotations can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Let $A = \\begin{bmatrix} a_1 & a_2 \\end{bmatrix}$. Then:<br>\n",
    "<br>\n",
    "$\\begin{bmatrix} a_1 & a_2 \\end{bmatrix}_A=\\begin{bmatrix} q_1 & q_2 \\end{bmatrix}_Q\n",
    "\\begin{bmatrix} a_1^Tq_1 & a_2^Tq_1 \\\\ a_1^Tq_2 & a_2^Tq_2 \\end{bmatrix}_R$<br>\n",
    "<br>\n",
    "Where $Q$ is unitary and $R$ is upper-triangular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INPUT : rectangular matrix A\n",
    "# OUTPUT: matrices Q - orthogonal and R - upper triangular such that A=QR\n",
    "def gram_schmidt(A): # 5 pts\n",
    "    # enter your code here\n",
    "    Q = np.zeros(A.shape)\n",
    "    q_1 = A[:,0].copy()\n",
    "    q_1 = q_1/np.linalg.norm(q_1)\n",
    "    Q[:,0] = q_1\n",
    "    for each_column in range(1,A.shape[1]):\n",
    "        q = A[:,each_column].copy()\n",
    "        for i in range(0,each_column):\n",
    "            q = q - np.dot((np.dot(q,Q[:,i])),Q[:,i])\n",
    "        q /= np.linalg.norm(q)\n",
    "        Q[:,each_column] = q\n",
    "    R=np.dot(Q.T,A)\n",
    "    return np.matrix(Q), np.matrix(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create $100x100$ random matrix $B$ and perform QR decomposition with our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "B=np.random.rand(100,100)*10\n",
    "Q,R = gram_schmidt(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to know how is far our matrix from orthogonality. For this we will compute Gram matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gram-Schmidt error: 1.61493377299e-14\n"
     ]
    }
   ],
   "source": [
    "def gram_matrix(Q):\n",
    "    G = np.zeros(Q.shape)\n",
    "    Q = np.array(Q)\n",
    "    for i in range(Q.shape[1]):\n",
    "        for j in range(Q.shape[0]):\n",
    "            G[j,i] = np.dot(Q[:,j], Q[:,i])\n",
    "    return np.matrix(G)\n",
    "\n",
    "print('Gram-Schmidt error:',np.linalg.norm(gram_matrix(Q)-np.identity(100),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_Hilbert(dim):\n",
    "    i = j = np.arange(1,dim+1,1)\n",
    "    ii,jj = np.meshgrid(i,j)\n",
    "    G = ii+jj\n",
    "    G-=1\n",
    "    G_func=np.vectorize(lambda x: np.power(x,-1.))\n",
    "    return G_func(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gram-Schmidt error on Hilbert matrix: 0.999994762472\n"
     ]
    }
   ],
   "source": [
    "A = create_Hilbert(100)\n",
    "Q,R = gram_schmidt(A)\n",
    "print('Gram-Schmidt error on Hilbert matrix:',np.linalg.norm(gram_matrix(Q)-np.identity(100),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Householder QR (10 pts)\n",
    "\n",
    "* Implement Householder QR decomposition as a function ```householder_qr(A)``` which outputs ```Q,R```. Apply it to the matrix $B$ created above. Print out the error.\n",
    "\n",
    "\n",
    "* Apply it to the Hilbert matrix $A$ created in the first part of the problem and print out the error. Consider how stable is Householder compared to Gram-Schmidt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INPUT : rectangular matrix A\n",
    "# OUTPUT: matrices Q - orthogonal and R - upper triangular such that A=QR\n",
    "def householder_qr(A): # 7 pts\n",
    "    Hs = list()\n",
    "    Rs=list()\n",
    "    Rs.append(A)\n",
    "    def get_H_for_submatrix(A):\n",
    "        vec_v = A[:,0]\n",
    "        w = np.zeros(vec_v.shape)\n",
    "        w[0] = np.linalg.norm(vec_v)\n",
    "        v = w - vec_v\n",
    "        P = np.outer(v,v.T)/np.dot(v.T,v) #np.inner()\n",
    "        H=np.identity(P.shape[0])-2*P\n",
    "        return H\n",
    "    H_1=get_H_for_submatrix(A)\n",
    "    size=H_1.shape[0]\n",
    "    Hs.append(H_1)\n",
    "    Q=H_1.copy()\n",
    "    for each_column in range(1,A.shape[1]-1):\n",
    "        if each_column==1:\n",
    "            sub_matrix = np.dot(Hs[-1], Rs[-1])\n",
    "        else:\n",
    "            sub_matrix=Rs[-1]\n",
    "        H_ = get_H_for_submatrix(sub_matrix[each_column:,each_column:])\n",
    "        householder = np.identity(size)\n",
    "        householder[each_column:,each_column:] = H_.copy()\n",
    "        Hs.append(householder)\n",
    "        Rs.append(np.dot(householder,sub_matrix))\n",
    "        Q=np.dot(Q,householder)\n",
    "    return np.matrix(Q), np.matrix(Rs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Householder error: 7.55836474173e-15\n"
     ]
    }
   ],
   "source": [
    "Q, R = householder_qr(B)\n",
    "print('Householder error:',np.linalg.norm(gram_matrix(Q)-np.identity(100),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Householder error on Hilbert matrix: 7.87884820929e-15\n"
     ]
    }
   ],
   "source": [
    "B = create_Hilbert(100)\n",
    "Q,R = householder_qr(B)\n",
    "print('Householder error on Hilbert matrix:',np.linalg.norm(gram_matrix(Q)-np.identity(100),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 (Low-rank decompositions)\n",
    "\n",
    "## 45 pts\n",
    "\n",
    "## 1. Theoretical tasks (15 pts)\n",
    "\n",
    "* Prove that for any Hermitian matrix, singular values equal to absolute value of eigenvalues. Does this hold for a general matrix? Prove or provide a counterexample.\n",
    "\n",
    "\n",
    "* Find analytically a skeleton decomposition of the matrix of size $n\\times m$ with elements $a_{ij} = \\sin i + \\sin j$.\n",
    "\n",
    "\n",
    "* Let $A\\in\\mathbb{C}^{n\\times m}$ be of rank $r$ and let $A = U\\Sigma V^*$ be its SVD. Prove that $\\mathrm{im}(A^*) = \\mathrm{span}\\{v_1,\\dots, v_r\\}$, where $V = [v_1, \\dots, v_n]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 3.1 (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular values: [ 6.31204578  3.16512506  2.85307929]\n",
      "Lambdas:  [ 3.16512506  6.31204578  2.85307929]\n"
     ]
    }
   ],
   "source": [
    "herm = np.matrix([[2,2+1j,4],[2-1j,3,1j],[4,-1j,1]])\n",
    "\n",
    "ls,s = np.linalg.eig(herm)[0],np.linalg.svd(herm)[1]\n",
    "print('Singular values:',s)\n",
    "print('Lambdas: ',np.abs(ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $A \\in \\mathbb{C}^{n\\times n}$ is Hermitian, $A^*A = A^2$.\n",
    "\n",
    "Singular values $\\sigma_1 \\geq, \\dots, \\geq \\sigma_n \\geq 0 $ of $A$ are square roots of eigenvalues of $A^*A$, hence, $\\sigma_1^2,\\dots,\\sigma_n^2$ -- eigenvalues of $A^2$. If $\\lambda_1,\\dots,\\lambda_n$ are eigenvalues of $A$, then (if renumber lamdas) $\\lambda_i^2 = \\sigma_i^2, \\sigma_i \\geq 0, i = 1,\\dots,n \\Rightarrow \\left|\\lambda_i \\right| = \\sigma_i, i = 1\\dots,n.$\n",
    "\n",
    "It's doesn't hold in the general case, consider the following matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A$ is Hermitian, so $A^2=A^*A$<br>\n",
    "The eigenvalues of $A^2$ are squared singular values of $A$. Thus, $\\lambda_1,...,\\lambda_n=\\sigma_1^2,...,\\sigma_n^2$, where $\\lambda_i$ - eigenvalues of $A^2$ and $\\sigma_i$ - singular values of $A$ (both localed in the descending order).<br>\n",
    "$\\lambda_i^2=\\sigma_i^2, \\sigma_i\\geq0$, hence, $|\\lambda_i|=\\sigma_i, i = 1,...,n$<br>\n",
    "<br>\n",
    "However, it is not true for general case, consider the matrix:<br>\n",
    "<br>\n",
    "$\\begin{pmatrix}\n",
    "2 & 2 \\\\\n",
    "-1 & 1 \\\\\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular values: [ 2.82842712  1.41421356]\n",
      "Lambdas:  [ 2.  2.]\n"
     ]
    }
   ],
   "source": [
    "contr = np.matrix([[2,2],[-1,1]])\n",
    "\n",
    "ls,s = np.linalg.eig(contr)[0],np.linalg.svd(contr)[1]\n",
    "print('Singular values:',s)\n",
    "print('Lambdas: ',np.abs(ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 3.1 (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skeleton decomposition: $A = C {\\hat A^{-1}}R$<br>\n",
    "$rank(A)=2$, if we substract the first row from all others: <br>\n",
    "<br>\n",
    "$\\begin{pmatrix} \\sin1+\\sin1 & \\sin1+\\sin2 & ... & \\sin1+\\sin m\\\\\n",
    "                 \\sin2-\\sin1 & \\sin2-\\sin1 & ... & \\sin2-\\sin1 \\\\\n",
    "                 \\vdots    & \\vdots & \\ddots & \\vdots    \\\\\n",
    "                 \\sin n-\\sin1& \\sin n-\\sin1& ... & \\sin n-\\sin1\\\\\n",
    "                 \\end{pmatrix}$, we will see that the rank is exactly 2<br>\n",
    "<br>\n",
    "\n",
    "Thus, our $\\hat A = \n",
    "\\begin {pmatrix}\n",
    "2\\sin1 & \\sin 1 + \\sin 2\\\\\n",
    "\\sin 2 + \\sin 1 & 2\\sin 2\n",
    "\\end{pmatrix}\n",
    "$,<br>\n",
    "\n",
    "$\\hat A^{-1} = \\frac{1}{(\\sin 1 - \\sin 2)^2}\n",
    "\\begin{pmatrix}\n",
    "-2\\sin 2 & \\sin 1 + \\sin 2\\\\\n",
    "\\sin 2 + \\sin 1 & -2\\sin 1\n",
    "\\end{pmatrix}\n",
    "$<br>\n",
    "<br>\n",
    "So, let's compute the rest:<br>\n",
    "$R =\n",
    "\\begin{pmatrix}\n",
    "2\\sin 1 & \\sin 1 + \\sin 2 & \\dots & \\sin 1 + \\sin m\\\\\n",
    "\\sin 2 + \\sin 1 & \\sin 2 + \\sin 2 & \\dots & \\sin 2 + \\sin m\\\\\n",
    "\\end{pmatrix}\n",
    "$<br>\n",
    "\n",
    "$C = \n",
    "\\begin {pmatrix}\n",
    "2\\sin 1 & \\sin 1 + \\sin 2\\\\\n",
    "\\sin 2 + \\sin 1 & \\sin 2 + \\sin 2\\\\\n",
    "\\vdots & \\vdots\\\\\n",
    "\\sin n + \\sin 1 & \\sin n + \\sin 2 &\\\\\n",
    "\\end{pmatrix}\n",
    "$<br>\n",
    "$A = C {\\hat A^{-1}}R$, so we now $C$,$\\hat A^{-1}$, $R$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 3.1 (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $V=\\{v_1,...,v_n\\}$ - an orthonormal basis of eigenvectors of $A^*A$<br>\n",
    "<br>\n",
    "$Im(A)=span(Av_1,...,Av_n)=span(Av_1,...,Av_r)$<br>\n",
    "<br>\n",
    "However, since $(Av_i)^*(Av_j)=\\sigma^2v_i^*v_j=\\sigma^2_i\\delta_{ij}$<br>\n",
    "<br>\n",
    "So, $ImA^*=span(A*Av_1,...,A*Av_r)=span(v_1,...,v_r)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recommender system using SVD (30 pts)\n",
    "\n",
    "In this task you are asked to build a simple movie recommender system based on *collaborative filtering* approach and SVD.\n",
    "Collaborative filtering implies that you build recommendations based on the feedback of other users given in a matrix $\\mathbf{M}$ of users vs. movies. \n",
    "If a user $i$ watched a movie $j$ and rated it, say, as $3$ out of $5$, then the value $3$ is the corresponding matrix entry, i.e. $\\mathbf{M}_{i,j}=3$.\n",
    "If a user did not watch a movie, then we put $0$ as a matrix element, i.e. $\\mathbf{M}=0$. \n",
    "Hence, the matrix is sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Building the core of recommender (15 pts)\n",
    "\n",
    "Build representation of users and movies in the latent factors space with the help of SVD.\n",
    "\n",
    "* We test the SVD model on a [Movielens 10M](https://grouplens.org/datasets/movielens/) dataset. Download the dataset using python functions provided in the following [Jupyter notebook](movielens10m.ipynb).\n",
    "\n",
    "\n",
    "* Is it possible to use ```np.linalg.svd``` function to calculate SVD of the downloaded matrices on your laptop? Provide an estimate.\n",
    "\n",
    "\n",
    "* Implement function `tr_svd` so that it computes truncated SVD using `scipy.linalg.svds`:\n",
    "    * Be aware that `scipy` returns singular values in ascending order (see the [docs](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.linalg.svds.html)).\n",
    "    * Sort all your svd data in descending (by singular values) order without breaking the result.\n",
    "    \n",
    "\n",
    "* Fix the rank of approximation and compute truncated SVD with `tr_svd` of the training set of the dataset. Plot the obtained singular values. Can you tell from the plot whether the data has a low-rank structure? Give your intuition, why it happens?\n",
    "\n",
    "\n",
    "* Write the function `top_n` which takes user as a row of his/her ratings (including non-rated films, i.e. just a row from the train\\test set), integer number $N$ and returns array of indices which correspond to $N$ highest ratings. Use function `np.argsort()`.\n",
    "\n",
    "\n",
    "* Pick several users at random from the training set. Compare their top-10 films and top-10, suggested by your model ($A_k = U_k \\Sigma_k V_k^T$). Comment on the result. **Note:** you can run all tests in this task with $k=25$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Libraries and provided functions\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import wget\n",
    "from io import StringIO \n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import sparse\n",
    "import scipy.sparse.linalg\n",
    "from tqdm import tqdm # Very useful library to see progress bar during range iterations: just type `for i in tqdm(range(10)):`\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "\n",
    "def get_movielens_data(local_file=None):\n",
    "    '''Downloads movielens data, normalizes users and movies ids,\n",
    "    returns data in sparse CSR format.\n",
    "    '''\n",
    "    if not local_file:\n",
    "        print('Downloading data...')\n",
    "        zip_file_url = 'http://files.grouplens.org/datasets/movielens/ml-10m.zip'\n",
    "        zip_contents = wget.download(zip_file_url)\n",
    "        print('Done.')\n",
    "    else:\n",
    "        zip_contents = local_file\n",
    "    \n",
    "    print('Loading data into memory...')\n",
    "    with zipfile.ZipFile(zip_contents) as zfile:\n",
    "        zdata = zfile.read('ml-10M100K/ratings.dat').decode()\n",
    "        delimiter = ';'\n",
    "        zdata = zdata.replace('::', delimiter) # makes data compatible with pandas c-engine\n",
    "        ml_data = pd.read_csv(StringIO(zdata), sep=delimiter, header=None, engine='c',\n",
    "                                  names=['userid', 'movieid', 'rating', 'timestamp'],\n",
    "                                  usecols=['userid', 'movieid', 'rating'])\n",
    "    \n",
    "    # normalize indices to avoid gaps\n",
    "    ml_data['movieid'] = ml_data.groupby('movieid', sort=False).grouper.group_info[0]\n",
    "    ml_data['userid'] = ml_data.groupby('userid', sort=False).grouper.group_info[0]\n",
    "    \n",
    "    # build sparse user-movie matrix\n",
    "    data_shape = ml_data[['userid', 'movieid']].max() + 1\n",
    "    data_matrix = sp.sparse.csr_matrix((ml_data['rating'],\n",
    "                                       (ml_data['userid'], ml_data['movieid'])),\n",
    "                                        shape=data_shape, dtype=np.float64)\n",
    "    \n",
    "    print('Done.')\n",
    "    return data_matrix\n",
    "\n",
    "def split_data(data, test_ratio=0.2):\n",
    "    '''Randomly splits data into training and testing datasets. Default ratio is 80%/20%.\n",
    "    Returns datasets in namedtuple format for convenience. Usage:\n",
    "    \n",
    "    train_data, test_data = split_data(data_matrix)\n",
    "    \n",
    "    or\n",
    "    \n",
    "    movielens_data = split_data(data_matrix)\n",
    "    \n",
    "    and later in code: \n",
    "    \n",
    "    do smth with movielens_data.train \n",
    "    do smth with movielens_data.test\n",
    "    '''\n",
    "    \n",
    "    num_users = data.shape[0]\n",
    "    idx = np.zeros((num_users,), dtype=bool)\n",
    "    sel = np.random.choice(num_users, int(test_ratio*num_users), replace=False)\n",
    "    np.put(idx, sel, True)\n",
    "    \n",
    "    Movielens_data = namedtuple('MovieLens10M', ['train', 'test'])\n",
    "    movielens_data = Movielens_data(train=data[~idx, :], test=data[idx, :])\n",
    "    return movielens_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data...\n",
      "Done.\n",
      "Loading data into memory...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "data=get_movielens_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INPUT: A: scipy.sparse.csr_matrix (N_train x N_films), k - integer\n",
    "# OUTPUT: U - np.array (N_train x k), S - np.array (k x k), Vh - np.array (k x N_films)\n",
    "def tr_svd(A, k): # 5 pts\n",
    "    U, S, V = sparse.linalg.svds(A, k)\n",
    "    return U[:,::-1], np.diag(S[::-1]), V[::-1]\n",
    "\n",
    "# INPUT: user - np.array (N_films,), N - integer \n",
    "# OUTPUT: np.array (N,)\n",
    "def top_n(user, N): # 2 pts\n",
    "    try:\n",
    "        user = np.squeeze(np.asarray(user.todense()))\n",
    "    except:\n",
    "        pass\n",
    "    sorted_idx = np.flip(np.argsort(user),axis=0)[:N]\n",
    "    return np.array(sorted_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will need to store around $10^{11}$ elements, so it's not possible to use simple np.linalg.svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train, data_test = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_u,data_train_s,data_train_vh = tr_svd(data_train,k=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f53b9407438>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxQJeWRsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f53b9449b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.yscale('log')\n",
    "plt.scatter(np.arange(data_train_s.shape[0]),np.diag(data_train_s),color='r',marker='o')\n",
    "plt.title('SV')\n",
    "plt.xlabel('# Singular Value')\n",
    "plt.ylabel('Singular Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singular values after 15th are differ only slightly, so that's why our data does not have low-rank structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_1 = data_train[32].copy()\n",
    "user_2 = data_train[11].copy()\n",
    "user_3 = data_train[214].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma_s = np.diag(data_train_s)\n",
    "\n",
    "prediction_1 = data_train_u[32]@sigma_s\n",
    "#prediction_1 = scipy.sparse.csr_matrix(np.dot(prediction_1,data_train_vh))\n",
    "prediction_1 = scipy.sparse.csr_matrix(np.dot(prediction_1,data_train_vh))\n",
    "\n",
    "\n",
    "prediction_2 = data_train_u[11]@sigma_s\n",
    "#prediction_2 = scipy.sparse.csr_matrix(np.dot(prediction_2,data_train_vh))\n",
    "prediction_2 = np.dot(prediction_2,data_train_vh)\n",
    "\n",
    "\n",
    "prediction_3 = data_train_u[214]@sigma_s\n",
    "#prediction_3 = scipy.sparse.csr_matrix(np.dot(prediction_3,data_train_vh))\n",
    "prediction_3 = np.dot(prediction_3,data_train_vh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1 actual : [  44  176  612  218  220  403  244  149 1545 2008]\n",
      "User 1 predict: [[  24 1003   34 ...,  399 1481   42]\n",
      " [  26   22   74 ...,   19  140  213]\n",
      " [   7  133   14 ...,  193 1093  605]\n",
      " ..., \n",
      " [1926  329  998 ..., 1122   72  307]\n",
      " [ 195   24  605 ...,   74   18   94]\n",
      " [  64   60  798 ...,  178   24  177]] \n",
      "\n",
      "User 2 actual : [1432   96  139 1003  101 1481 1292  141 1426  133]\n",
      "User 2 predict: [[  24 1003   34 ...,  399 1481   42]\n",
      " [  26   22   74 ...,   19  140  213]\n",
      " [   7  133   14 ...,  193 1093  605]\n",
      " ..., \n",
      " [1926  329  998 ..., 1122   72  307]\n",
      " [ 195   24  605 ...,   74   18   94]\n",
      " [  64   60  798 ...,  178   24  177]] \n",
      "\n",
      "User 3 actual : [ 382   24 1351 1007  993  810  616  610  605  277]\n",
      "User 3 predict: [[  42 1481  399 ...,   34 1003   24]\n",
      " [ 213  140   19 ...,   74   22   26]\n",
      " [ 605 1093  193 ...,   14  133    7]\n",
      " ..., \n",
      " [ 307   72 1122 ...,  998  329 1926]\n",
      " [  94   18   74 ...,  605   24  195]\n",
      " [ 177   24  178 ...,  798   60   64]]\n"
     ]
    }
   ],
   "source": [
    "#flip\n",
    "print('User 1 actual :',top_n(user_1,10))\n",
    "print('User 1 predict:',top_n(prediction_1,10), '\\n')\n",
    "\n",
    "print('User 2 actual :',top_n(user_2,10))\n",
    "print('User 2 predict:',top_n(prediction_2,10), '\\n')\n",
    "\n",
    "print('User 3 actual :',top_n(user_3,10))\n",
    "print('User 3 predict:',top_n(prediction_3,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Evaluating performance of the recommender (15 pts)\n",
    "\n",
    "Suppose, we trained our model (obtain $U_k, \\Sigma_k, V^T_k$ from $A_{train}$). Let's evaluate it! For this purpose we have $A_{test}$ (recall the function [```split_data```](movielens10m.ipynb)). And our goal is to obtain vectors of recommendation $r$ for each user (row) in the test set ($A_{test}$). But there is no need to recompute the whole SVD for each user. We have the tool, which is called _folding-in_ for recommender systems.\n",
    "\n",
    "#### Folding-in technique                                                             \n",
    "\n",
    "<img src=\"decomp.png\" width='450'>\n",
    "\n",
    "\n",
    "A new user can be considered as an update to the original matrix (appending new row). Appending a row in the original matrix corresponds to appending a row into the users latent factors matrix $U_k$ in the SVD decomposition. \n",
    "Since we do not want to recompute the SVD, we project the new user on the space of found latent factors $V_k$, which spans the row space of matrix $A_k = U_k \\Sigma_k V^T_k$ (recall the problem from the theoretical tasks).\n",
    "The orthoprojection on this space is $P = V_kV_k^T$ (check that it satisfies definition of orthoprojection, i.e. $P^2=P$, $P^T = P$).\n",
    "\n",
    "Thus, the recommendation vector $r$ for a new user $x$ (considered as a column vector) can be written as\n",
    "\n",
    "$$\n",
    "r = V_kV_k^T x.\n",
    "$$\n",
    "\n",
    "\n",
    "#### Computing the total score\n",
    "You have to iterate over all users in the test set and make the following steps:\n",
    "* obtain vector $x$, which is the same as user row, but the last $N = 3$ rated films should be filled with zeroes. Example:\n",
    "\n",
    "$$\n",
    "user = (0, 0, 1, 3, 5, 2, 0, 2, 2, 1, 0, 5) \\;\\; \\to \\;\\;  x = (0, 0, 1, 3, 5, 2, 0, 2, 2, 0, 0, 0).\n",
    "$$\n",
    "\n",
    "* compute the folding-in prediction $r$:\n",
    "\n",
    "$$\n",
    "r = V_k V_k^T x.\n",
    "$$\n",
    "\n",
    "* Obtain top-3 from $user$ (truth) and top-3 from $r$ (prediction). The number of films appearing _simultaneously_ in both top-3's  should be added to the `total_score`. Write the corresponding function `total_score_folding`, which takes the sparse test matrix $A_{test}$,  $V_k$ from truncated SVD of $A_{train}$ and compute the total score. \n",
    "\n",
    "**Example: **\n",
    "\n",
    "|    $user$    |     $recommendation$    |\n",
    "|:------------:|:----------:|\n",
    "|    (**1**,**2**,3)   |  (10,**2**,**1**)  |\n",
    "| (34, 27, **69**) | (**69**, 5, 9) |\n",
    "|    (7,6,4)   |   (8,9,2)  |\n",
    "\n",
    "```total_score``` = 2 + 1 + 0 = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INPUT: V - np.array(N_films, k), test_data - scipy.sparse.csr_matrix (N_train x N_films)\n",
    "# OTPUT: total_score - integer\n",
    "def total_score_folding(V, test_data): # 8 pts\n",
    "    # enter you code here\n",
    "    total_score = 0\n",
    "    test_data[:,-3:]=scipy.sparse.csr_matrix(np.zeros((test_data.shape[0],3)))\n",
    "    for user in test_data:\n",
    "        rec = V@user.T\n",
    "        rec = scipy.sparse.csr_matrix(V.T@rec)\n",
    "        total_score+=np.intersect1d(top_n(user,3), top_n(rec,3)).shape[0]\n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score: 4596\n"
     ]
    }
   ],
   "source": [
    "t_score = total_score_folding(V=data_train_vh,test_data=data_test)\n",
    "print('Total score:', t_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.70137231e-03,   3.26203879e-02,   4.19550319e-02, ...,\n",
       "          1.25646355e-05,   1.29603590e-05,  -1.57938956e-20],\n",
       "       [  1.48838056e-02,   9.53208412e-02,   7.12418664e-02, ...,\n",
       "         -2.81169427e-05,  -1.33436335e-05,  -5.01850572e-18],\n",
       "       [  2.62027643e-03,  -1.19929910e-02,  -4.22761401e-02, ...,\n",
       "          2.89690922e-05,  -1.29500411e-05,  -3.15189953e-18],\n",
       "       ..., \n",
       "       [  4.50436992e-03,   4.55524331e-02,   1.91466872e-02, ...,\n",
       "         -1.04307765e-04,   1.47424851e-04,   2.92922293e-17],\n",
       "       [  1.17963598e-02,  -1.23311125e-02,  -7.61914235e-04, ...,\n",
       "         -9.20762261e-05,  -5.03158933e-05,  -1.90151904e-17],\n",
       "       [  8.35188081e-03,   2.68575502e-03,  -1.23820081e-02, ...,\n",
       "          8.57729361e-05,  -6.57212126e-05,   1.29121456e-17]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_vh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (bonus) Fine-tuning your model\n",
    "\n",
    "* Try to find the rank that produces the best evaluation score.\n",
    "    * Plot the dependency of evaluation score on the rank of SVD for all your trials in one graph.\n",
    "* Report the best result and the corresponding SVD rank.\n",
    "* Compare your model with the non-personalized recommender which simply recommends top-3 movies with highest average ratings. \n",
    "\n",
    "**Note**, that you don't have to recompute SVD to evaluate your model. You might compute once relatively large ($k =500$) truncated SVD and then just use submatrices of it.\n",
    "\n",
    "**Optionally:**\n",
    "You may want to test your parameters with different data splittings in order to minimize the risk of local effects.\n",
    "You're also free to add modifications to your code for producing better results. Report what modificatons you've done and what effect it had if any."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 (eigenvalues)\n",
    "\n",
    "## 55 pts\n",
    "\n",
    "### 1. Theoretical tasks (10 pts)\n",
    "\n",
    "* Prove that normal matrix is Hermitian iff its eigenvalues are real. Prove that normal matrix is unitary iff its eigenvalues satisfy $|\\lambda| = 1$. \n",
    "\n",
    "* The following problem illustrates instability of the Jordan form. Find theoretically the eigenvalues of the perturbed Jordan block:\n",
    "\n",
    "$$\n",
    "    J(\\varepsilon) = \n",
    "    \\begin{bmatrix} \n",
    "     \\lambda & 1 & & & 0 \\\\ \n",
    "     & \\lambda & 1 & & \\\\ \n",
    "     &  & \\ddots & \\ddots & \\\\ \n",
    "     & & & \\lambda & 1 \\\\ \n",
    "     \\varepsilon & & & & \\lambda  \\\\ \n",
    "    \\end{bmatrix}_{n\\times n}\n",
    "$$\n",
    "\n",
    "Comment how eigenvalues of $J(0)$ are perturbed for large $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 4.1 (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All eigenvalues are real?<br>\n",
    "<br>\n",
    "$Hv=\\lambda v$<br> \n",
    "Let's take conjugate transpose: $(Hv)^* = \\lambda^*v^* = v^*H^* = \\lambda^*v^*$ and multiply it by $v$:<br>\n",
    "$v^*H^*v=\\lambda^*v^*v$, hence, because of non eigenvector is zero:<br>\n",
    "$v^*\\lambda v = \\lambda^*v^*v\\Rightarrow \\lambda^*=\\lambda$, thus all eigenvalues are real.<br>\n",
    "<br>\n",
    "- Normal matrix is unitary iff its eigenvalues satisfy $|\\lambda| = 1?$<br>\n",
    "<br>\n",
    "$(Uv)^*(Uv)=(\\lambda v)^*(\\lambda v)=|\\lambda|v^*v$<br>\n",
    "Simultaneously, $(Uv)^*(Uv)=(U^*Uv)^*v=v^*v,$ so $|\\lambda|=1$<br>\n",
    "Consider $V=\\{v_1,...,V_n\\}$ - an orthonormal basis from the eigenvectores of $A$. Thus, we can express any vector $x$ as \n",
    "$\\sum_{i=1}^n c_iv_i$, hence $Ux=c_1\\lambda_1v_1+...+c_n\\lambda_nv_n$<br>\n",
    "$x^*x=|c_1|^2+...+|c_n|^2 \\Rightarrow (Ux)^*(Ux)=x*x,$ hence $U$ is unitary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 4.1 (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the $det J(\\epsilon)$ trough expansion: $detJ(\\epsilon)=(-1)^{n-1}\\epsilon+\\lambda^n$<br>\n",
    "<br>\n",
    "We can expres eigenvalues of $J(\\epsilon):(\\lambda-\\Lambda)^n=(-1)^n\\epsilon$, hence: <br>\n",
    "<br>\n",
    "$\\Lambda=\\lambda+\\epsilon^{-n}$<br>\n",
    "<br>\n",
    "The number of different solutions (complex solutions) for $y=\\epsilon^{-n}$ is $n$<br>\n",
    "<br>\n",
    "The eigenvalues of pertubated matrices are on the complex plane on the circle with radius $=|\\epsilon|^{-n}$ and center in $\\lambda$, so for large n (let's consider $n\\to\\infty$) the circle shrinks and all eigenvalues strive to $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PageRank (30 pts)\n",
    "\n",
    "\n",
    "#### Damping factor importance\n",
    "\n",
    "* Write the function ```pagerank_matrix(G)``` that takes an adjacency matrix $G$ as an input and outputs the corresponding PageRank matrix $A$.\n",
    "\n",
    "* Find PageRank matrix $A$ that corresponds to the following graph: <img src=\"graph.png\" width='250'>\n",
    "What is its largest eigenvalue? What multiplicity does it have?\n",
    "\n",
    "\n",
    "* Implement the power method for a given matrix $A$, an initial guess $x_0$ and a number of iterations ```num_iter```. It should be organized as a function ```power_method(A, x0, num_iter)``` that outputs approximation to eigenvector $x$, eigenvalue $\\lambda$ and history of residuals $\\{\\|Ax_k - \\lambda_k x_k\\|_2\\}$. Make sure that the method conveges to the correct solution on a matrix $\\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix}$ which is known to have the largest eigenvalue equal to $3$.\n",
    "\n",
    "\n",
    "* Run the power method for the graph presented above and plot residuals $\\|Ax_k - \\lambda_k x_k\\|_2$ as a function of $k$ for ```num_iter=100``` and random initial guess ```x0```.  Explain the absence of convergence. \n",
    "\n",
    "\n",
    "* Consider the same graph, but with the directed edge that goes from the node 3 to the node 4 being removed. Plot residuals as in the previous task and discuss the convergence. Now, run the power method with ```num_iter=100``` for 10 different initial guesses and print/plot the resulting approximated eigenvectors. Why do they depend on the initial guess?\n",
    "\n",
    "\n",
    "In order to avoid this problem Larry Page and Sergey Brin [proposed](http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf) to use the following regularization technique:\n",
    "\n",
    "$$\n",
    "A_d = dA + \\frac{1-d}{N} \\begin{pmatrix} 1 & \\dots & 1 \\\\ \\vdots & & \\vdots \\\\ 1 & \\dots & 1 \\end{pmatrix},\n",
    "$$\n",
    "\n",
    "where $d$ is a small parameter in $[0,1]$ (typically $d=0.85$), which is called **damping factor**, $A$ is of size $N\\times N$. Now $A_d$ is the matrix with multiplicity of the largest eigenvalue equal to 1. \n",
    "Recall that computing the eigenvector of the PageRank matrix, which corresponds to the largest eigenvalue, has the following interpretation. Consider a person who stays in a random node of a graph (i.e. opens a random web page); at each step s/he follows one of the outcoming edges uniformly at random (i.e. opens one of the links). So the person randomly walks through the graph and the eigenvector we are looking for is exactly his/her stationary distribution — for each node it tells you the probability of visiting this particular node. Therefore, if the person has started from a part of the graph which is not connected with the other part, he will never get there.  In the regularized model, the person at each step follows one of the outcoming links with probability $d$ OR visits a random node from the whole graph with probability $(1-d)$.\n",
    "\n",
    "* Now, run the power method with $A_d$ and plot residuals $\\|A_d x_k - \\lambda_k x_k\\|_2$ as a function of $k$ for $d=0.99$, ```num_iter=100``` and a random initial guess ```x0```.\n",
    "\n",
    "\n",
    "Usually, graphs that arise in various areas are sparse (social, web, road networks, etc.) and, thus, computation of a matrix-vector product for corresponding PageRank matrix $A$ is much cheaper than $\\mathcal{O}(N^2)$. However, if $A_d$ is calculated directly, it becomes dense and, therefore, $\\mathcal{O}(N^2)$ cost grows prohibitively large for  big $N$.\n",
    "\n",
    "\n",
    "* Implement fast matrix-vector product for $A_d$ as a function ```pagerank_matvec(A, d, x)```, which takes a PageRank matrix $A$ (in sparse format, e.g., ```csr_matrix```), damping factor $d$ and a vector $x$ as an input and returns $A_dx$ as an output. Generate a random adjacency matrix of size $10000 \\times 10000$ with only 100 non-zero elements and compare ```pagerank_matvec``` performance with direct evaluation of $A_dx$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$G = \\begin{pmatrix}\n",
    "0 & 0 & 1 & 0 & 0\\\\\n",
    "1 & 0 & 1 & 0 & 0\\\\\n",
    "0 & 1 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 1\\\\\n",
    "0 & 0 & 0 & 1 & 0\\\\\n",
    "\\end{pmatrix}$ is just an adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = np.array([[0,0,1,0,0],[1,0,1,0,0],[0,1,0,0,0],[0,0,0,0,1],[0,0,0,1,0]],dtype=np.float32)\n",
    "G = np.ndarray(shape=G.shape, buffer = G, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INPUT:  G - np.ndarray\n",
    "# OUTPUT: A - np.ndarray (of size G.shape)\n",
    "def pagerank_matrix(G): # 5 pts\n",
    "    # enter your code here\n",
    "    A = G.copy()\n",
    "    for each_column in range(A.shape[1]):\n",
    "        if bool(np.sum(A[:,each_column])):\n",
    "            A[:,each_column] = A[:,each_column]/np.sum(A[:,each_column])\n",
    "        else:\n",
    "            A[:,each_column] = 0.\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   0.   0.5  0.   0. ]\n",
      " [ 1.   0.   0.5  0.   0. ]\n",
      " [ 0.   1.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   1. ]\n",
      " [ 0.   0.   0.   1.   0. ]]\n"
     ]
    }
   ],
   "source": [
    "A=pagerank_matrix(G)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A$ has the largest eigenvalue of 1 with the multiplicity of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x0 = np.ones((1,A.shape[0])) / A.shape[0]\n",
    "x0 = np.ndarray(shape=x0.shape, buffer=x0, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x0 = np.random.rand(1,A.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INPUT:  A - np.ndarray (2D), x0 - np.ndarray (1D), num_iter - integer (positive) \n",
    "# OUTPUT: x - np.ndarray (of size x0), l - float, res - np.ndarray (of size num_iter + 1 [include initial guess])\n",
    "def power_method(A, x0, num_iter):\n",
    "    # enter your code here\n",
    "    l= list()\n",
    "    l.append(np.max(A@x0.T))\n",
    "    \n",
    "    res = np.zeros(num_iter+1, dtype = np.float32)\n",
    "    res = np.ndarray(shape=res.shape, buffer=res, dtype=np.float32)\n",
    "    res[0] = np.linalg.norm(A@x0.T-l[-1]*x0,2)\n",
    "    for i in range(num_iter):\n",
    "        try:\n",
    "            x1 = np.dot(A, x0.T)\n",
    "        except:\n",
    "            x1 = np.dot(A, x0)\n",
    "        l.append(np.max(x1))\n",
    "        try:\n",
    "            #res.append(np.linalg.norm(A@x1 - l[-1]*x1,2))\n",
    "            res[i+1] = np.linalg.norm(A@x1 - l[-1]*x1,2)\n",
    "        except:\n",
    "            #res.append(np.linalg.norm(A@x1.T - l[-1]*x1,2))\n",
    "            res[i+1] = np.linalg.norm(A@x1.T - l[-1]*x1,2)\n",
    "        x0 = x1 / l[-1]\n",
    "        if np.linalg.matrix_rank(A) < A.shape[0]:\n",
    "            if bool(np.sum(np.isnan(x0))):\n",
    "                x0 = np.zeros(x0.shape, dtype=np.float32)\n",
    "                x0 = np.ndarray(shape=x0.shape, buffer = x0,dtype=np.float32)\n",
    "                l[-1] = 0.\n",
    "    return x0,l[-1], res #eigenvecor, eigenvalue, residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.array([[2,-1],[-1,2]])\n",
    "x0 = np.random.rand(1,A.shape[0])\n",
    "x0 = np.ndarray(shape=x0.shape, buffer=x0, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "check=power_method(A, x0, num_iter=100)\n",
    "print(check[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(111)\n",
    "A=pagerank_matrix(G)\n",
    "x0 = np.random.rand(1,A.shape[0])\n",
    "x0 = np.ndarray(shape=x0.shape, buffer=x0, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_approximated, l_approximated, res_approximated = power_method(A, x0, num_iter=100)\n",
    "x_approximated, l_approximated, res_approximated = power_method(A, x0, num_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.41421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f53c2782e10>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X+8VXWd7/HX++yzzw8ENQEVAQWJyZQUmxOa402cfmGj\nUvc2c6GamiZjbNKpyZrRmcfDpprm0Vx7zFipERmRt9SstBjDH9mUml7MgxHiD4xA5ZDKEfyFKHA4\nn/vHWgcWm703+8BZZx/2eT8fj/Ngr5/7s8jOm+/3u9b6KiIwMzPbm6Z6F2BmZgcGB4aZmdXEgWFm\nZjVxYJiZWU0cGGZmVhMHhpmZ1cSBYTZAJD0kaWaFbTMldQ3Q9/xS0nkDcS6z/miudwFm9SDpceAI\nYAewGbgVuCAiNu/rOSPihIGpzmxocgvDhrNzImIkMB04GbikzvWYDWkODBv2IuJp4DaS4EBSq6Qv\nS3pS0jOS5ktqT7eNkXSzpOclbZJ0t6SmdNvjkt6Wfm6XtEjSc5IeBt6U/U5JIem1meVFkv41/fya\n9Du60+NvljShXO2SXivpTkkvSHpW0vdz+CsyAxwYZqS/jM8CVqervgT8EUmAvBYYD1yabrsI6ALG\nknRp/RNQ7v06nwWmpD/vBD7Uj5KagG8DxwBHA68AV1TY9wvA7cBrgAnA1/rxPWb90nCBIWmhpA2S\nVtaw76ckPSxphaSfSzoms22HpOXpz+J8q7Y6+bGkl4B1wAbgs5IEzAP+PiI2RcRLwL8Bc9JjtgPj\ngGMiYntE3B3lX8j2F8AX03OsA75aa1ERsTEifhQRW9Lv/yJwRoXdt5MEy1ER8WpE/KrW7zHrr4YL\nDGARMKvGfX8DdETEicAPgf+T2fZKRExPf84d4BptaHh3RIwCZgLHAWNIWg4jgGVpt9PzJAPiY9Nj\nLiNpidwuaY2kiyuc+yiSIOrzRK1FSRoh6RuSnpD0InAXcKikQpnd/wEQ8Ov0Lq2/rvV7zPqr4QIj\nIu4CNmXXSZoi6VZJy9I+5+PSfX8REVvS3ZaSNOltmImIO0n+ofFl4FmSLqATIuLQ9OeQdHCciHgp\nIi6KiGOBc4FPSXprmdM+BUzMLB9dsn0LSTD1OTLz+SLgdcApEXEw8JZ0vcrU/nREfDQijgL+Brgq\nOzZiNpAaLjAqWABcGBF/DHwauKrMPh8Bbskst0l6QNJSSe8ejCKtri4H3g68Afgm8J+SDgeQNF7S\nO9PPZ6cDzQJeILktt7fM+W4ALkkHsCcAF5ZsXw68T1JB0ix273IaRRJaz0s6jGQ8pCxJf54ZEH+O\nZDylXD1m+63hA0PSSOA04AeSlgPfIOmDzu7zAaCDpLuhzzER8UbgfcDlkqYMUslWBxHRDVxDMrj9\njyTdTkvTLqE7SP7FDzA1Xd4M/D/gqoj4RZlTfo6kG2otyaD0/y3Z/gngHOB54P3AjzPbLgfaSVo7\nS0m6xCp5E3CfpM3AYuATEbGmhks26zc14gRKkiYBN0fENEkHA6siYlyFfd9GcmfJGRGxocI+i9Lz\n/TCfis3Mhr6Gb2FExIvAWkl/DqDESennk0laHOdmwyLtRmhNP48B/gR4eNCLNzMbQhquhSHpOpK7\nXsYAz5D0//438HWSrqgicH1EfF7SHSR91k+lhz8ZEedKOo0kSHpJQvXyiPjWoF6ImdkQ03CBYWZm\n+Wj4LikzMxsYDfW22jFjxsSkSZPqXYaZ2QFj2bJlz0bE2L3v2WCBMWnSJDo7O+tdhpnZAUNSzW8h\ncJeUmZnVxIFhZmY1cWCYmVlNHBhmZlYTB4aZmdUkt7ukJC0EzgY2RMS0Mts/Q/LStb46Xg+MjYhN\nkh4HXiJ5E2hPRHTkVaeZmdUmzxbGIqpMZBQRl/VNUARcAtwZEdl5LM5MtzsszMyGgNwCo9xERlXM\nBa7Lq5a9+erPf8edj3XX6+vNzA4IdR/DkDSCpCXyo8zqAO5IZ8ibt5fj50nqlNTZ3b1vv/S/cefv\nuduBYWZWVd0Dg2QSmXtKuqNOT7uqzgI+Lukt5Q+FiFgQER0R0TF2bE1Pt++hrVjg1Z4d+3Ssmdlw\nMRQCYw4l3VERsT79cwNwEzAjzwLaigVe3e5ZLc3MqqlrYEg6hGQu459k1h0kaVTfZ+AdwMo862ht\nbuLV7W5hmJlVk+dttTsnMpLURTKRUREgIuanu70HuD0iXs4cegRwk6S++q6NiGpzGu+3VrcwzMz2\nKrfAiIi5NeyziOT22+y6NcBJ+VRVXluxia0ewzAzq2oojGHUXVtzga1uYZiZVeXAIGlh+C4pM7Pq\nHBj03SXlwDAzq8aBgW+rNTOrhQODtEvKLQwzs6ocGEBrs7ukzMz2xoEBtBabeLXHXVJmZtU4MEhu\nq93W00tvb9S7FDOzIcuBQTLoDbDVrQwzs4ocGCSD3oDHMczMqnBg4BaGmVktHBi4hWFmVgsHBsmg\nN+DXg5iZVeHAYFeXlJ/2NjOrzIFBMoESuEvKzKwaBwbJBErgwDAzq8aBQXbQ211SZmaVODDI3lbr\nFoaZWSUODLKD3g4MM7NKcgsMSQslbZC0ssL2mZJekLQ8/bk0s22WpFWSVku6OK8a+7Slg95+cM/M\nrLI8WxiLgFl72efuiJie/nweQFIBuBI4CzgemCvp+BzrdAvDzKwGuQVGRNwFbNqHQ2cAqyNiTURs\nA64HZg9ocSV23VbrFoaZWSX1HsM4TdIKSbdIOiFdNx5Yl9mnK11XlqR5kjoldXZ3d+9TEc2FJpqb\n5BaGmVkV9QyMB4CjI+JE4GvAj/flJBGxICI6IqJj7Nix+1yM5/U2M6uuboERES9GxOb08xKgKGkM\nsB6YmNl1QrouV23FJr9LysysiroFhqQjJSn9PCOtZSNwPzBV0mRJLcAcYHHe9XhebzOz6przOrGk\n64CZwBhJXcBngSJARMwH3gt8TFIP8AowJyIC6JF0AXAbUAAWRsRDedXZp63YxFZ3SZmZVZRbYETE\n3L1svwK4osK2JcCSPOqqJBnDcAvDzKySet8lNWS0FQsewzAzq8KBkXKXlJlZdQ6MVFuzWxhmZtU4\nMFKtxSY/h2FmVoUDI9Xm22rNzKpyYKRa/aS3mVlVDoxUMujtFoaZWSUOjJRvqzUzq86BkWprLrB9\nR7CjN+pdipnZkOTASLUV++bEcCvDzKwcB0aqb9Y9T9NqZlaeAyO1a9Y9tzDMzMpxYKQ8r7eZWXUO\njNSuMQx3SZmZlePASLX2tTB8a62ZWVkOjFRbs7ukzMyqcWCk+rqk/IpzM7PyHBgpD3qbmVXnwEi1\neQzDzKyq3AJD0kJJGyStrLD9/ZJWSHpQ0r2STspsezxdv1xSZ141ZrlLysysujxbGIuAWVW2rwXO\niIg3AF8AFpRsPzMipkdER0717abVg95mZlU153XiiLhL0qQq2+/NLC4FJuRVSy12PofhV4OYmZU1\nVMYwPgLcklkO4A5JyyTNq3agpHmSOiV1dnd373MBvq3WzKy63FoYtZJ0JklgnJ5ZfXpErJd0OPAz\nSY9GxF3ljo+IBaTdWR0dHfv8bvKmJtFS8LzeZmaV1LWFIelE4GpgdkRs7FsfEevTPzcANwEzBqOe\n1mKTWxhmZhXULTAkHQ3cCPxlRDyWWX+QpFF9n4F3AGXvtBpobcUCW31brZlZWbl1SUm6DpgJjJHU\nBXwWKAJExHzgUmA0cJUkgJ70jqgjgJvSdc3AtRFxa151ZrUV3SVlZlZJnndJzd3L9vOA88qsXwOc\ntOcR+WtrLrhLysysgqFyl9SQ4DEMM7PKHBgZbc0FT9FqZlaBAyOjreguKTOzShwYGR70NjOrzIGR\n0Vos+G21ZmYVODAy2poLflutmVkFDoyMNt8lZWZWkQMjw4PeZmaVOTAyWpub/HpzM7MKHBgZbcUC\nO3qD7TscGmZmpRwYGTunaXUrw8xsDw6MjLaiJ1EyM6vEgZHhWffMzCpzYGS09s3r7WcxzMz24MDI\ncJeUmVllDoyMvsDwrHtmZntyYGS0NbtLysysEgdGRqu7pMzMKnJgZLR50NvMrKLcAkPSQkkbJK2s\nsF2SvipptaQVkt6Y2TZL0qp028V51VjKt9WamVWWZwtjETCryvazgKnpzzzg6wCSCsCV6fbjgbmS\njs+xzp12DXq7hWFmViq3wIiIu4BNVXaZDVwTiaXAoZLGATOA1RGxJiK2Aden++ZuV5eUWxhmZqXq\nOYYxHliXWe5K11Van7udz2H4tlozsz0c8IPekuZJ6pTU2d3dvV/navVttWZmFdUzMNYDEzPLE9J1\nldaXFRELIqIjIjrGjh27XwVJorW5ia3ukjIz20M9A2Mx8MH0bqlTgRci4ingfmCqpMmSWoA56b6D\norXZ07SamZXTvLcdJB0B/BtwVEScld6x9OaI+NZejrsOmAmMkdQFfBYoAkTEfGAJ8C5gNbAF+HC6\nrUfSBcBtQAFYGBEP7dvl9d+IlmZ3SZmZlbHXwCC5PfbbwD+ny48B3weqBkZEzN3L9gA+XmHbEpJA\nGXTtLQW2uIVhZraHWrqkxkTEDUAvJC0AoGF/o7YVC7yyrWEvz8xsn9USGC9LGg0EQN94Q65V1VF7\n0WMYZmbl1NIl9SmSQecpku4BxgLvzbWqOmpvKfCKA8PMbA97DYyIeEDSGcDrAAGrImJ77pXVSXux\nmedefqXeZZiZDTm13CX1wZJVb5RERFyTU0115RaGmVl5tXRJvSnzuQ14K/AA0JiBUWzyoLeZWRm1\ndEldmF2WdCjJCwEbUnvRLQwzs3L25Unvl4HJA13IUNHe0uzAMDMro5YxjP8ivaWWJGCOB27Is6h6\nai8W2NbTy47eoNCkepdjZjZk1DKG8eXM5x7giYjoyqmeumtvSRpdr2zfwcjWWv56zMyGh1rGMO4c\njEKGivZ0ToxXtjkwzMyyKv5GlPQSu7qidttE8iqog3Orqo7aW5K/Ej/tbWa2u4qBERGjBrOQoaKv\nhbHFt9aame2m5j4XSYeTPIcBQEQ8mUtFdZYdwzAzs132elutpHMl/Q5YC9wJPA7cknNdddOWGcMw\nM7NdankO4wvAqcBjETGZ5EnvpblWVUcjPIZhZlZWLYGxPSI2Ak2SmiLiF0BHznXVjccwzMzKq2UM\n43lJI4G7ge9J2kDytHdD2nlbrVsYZma7qdjCkHSlpNOB2SRzbn8SuBX4PXDO4JQ3+No86G1mVla1\nFsZjwGXAOJJXgVwXEd8ZlKrqaOcYhrukzMx2U7GFERFfiYg3A2cAG4GFkh6VdKmkP6rl5JJmSVol\nabWki8ts/4yk5enPSkk7JB2Wbntc0oPpts59vL5+a2t2C8PMrJy9DnpHxBMR8e8RcTIwF3gP8Mje\njpNUAK4EziJ5YeFcSceXnPuyiJgeEdOBS4A7I2JTZpcz0+2DNsjeXGiipdDkQW8zsxK1PIfRLOkc\nSd8jef5iFfA/azj3DGB1RKyJiG0kc2jMrrL/XOC6Gs6bu7Zik2+rNTMrUW3Q++2SFgJdwEeBnwJT\nImJORPykhnOPB9ZllrvSdeW+awQwC/hRZnUAd0haJmlelTrnSeqU1Nnd3V1DWXs3oqXZD+6ZmZWo\nNuh9CXAtcFFEPJdzHecA95R0R50eEevTV5L8TNKjEXFX6YERsQBYANDR0VHuZYn95nm9zcz2VO3l\ng3+6n+deD0zMLE9I15Uzh5LuqIhYn/65QdJNJF1cewRGHtqKBY9hmJmV2JcpWmt1PzBV0mRJLSSh\nsLh0J0mHkNyJ9ZPMuoMkjer7DLwDWJljrbtp9xiGmdkecpshKCJ6JF0A3AYUgIUR8ZCk89Pt89Nd\n3wPcHhHZp8ePAG6S1FfjtRFxa161lhrheb3NzPaQ65RyEbEEWFKybn7J8iJgUcm6NcBJedZWTVux\nwKaXt9Xr683MhqQ8u6QOWB70NjPbkwOjjPZik2+rNTMr4cAow2MYZmZ7cmCU0VZ0l5SZWSkHRhnt\nxQLbenrZ0TsgzwGamTUEB0YZ7Z4Tw8xsDw6MMtrTOTE88G1mtosDo4y+aVr9tLeZ2S4OjDI8r7eZ\n2Z4cGGX0jWH4BYRmZrs4MMpoL3oMw8yslAOjjPYWj2GYmZVyYJThMQwzsz05MMroCwyPYZiZ7eLA\nKKOvS8otDDOzXRwYZewcw3ALw8xsJwdGGW3NfjWImVkpB0YZzYUmWgpNDgwzswwHRgVtnkTJzGw3\nuQaGpFmSVklaLeniMttnSnpB0vL059Jaj83biJZmB4aZWUZzXieWVACuBN4OdAH3S1ocEQ+X7Hp3\nRJy9j8fmxvN6m5ntLs8WxgxgdUSsiYhtwPXA7EE4dkB41j0zs93lGRjjgXWZ5a50XanTJK2QdIuk\nE/p5LJLmSeqU1Nnd3T0QdQPQ7jEMM7Pd1HvQ+wHg6Ig4Efga8OP+niAiFkRER0R0jB07dsAKG9HS\n7BaGmVlGnoGxHpiYWZ6QrtspIl6MiM3p5yVAUdKYWo7NW1ux4BaGmVlGnoFxPzBV0mRJLcAcYHF2\nB0lHSlL6eUZaz8Zajs1be0vBb6s1M8vI7S6piOiRdAFwG1AAFkbEQ5LOT7fPB94LfExSD/AKMCci\nAih7bF61ltNebPLLB83MMnILDNjZzbSkZN38zOcrgCtqPXYweQzDzGx39R70HrJ8W62Z2e4cGBW0\nFwts6+llR2/UuxQzsyHBgVFBe4vfWGtmluXAqKC9JRne8a21ZmYJB0YFfdO0+tZaM7OEA6OCvsBw\nl5SZWcKBUcHOMQx3SZmZAQ6MitqLyRiGH94zM0s4MCpob/EYhplZlgOjAo9hmJntzoFRwc7AcJeU\nmRngwKior0tqi1sYZmaAA6OinWMYbmGYmQEOjIramv1qEDOzLAdGBc2FJloKTQ4MM7OUA6OK9hZP\n02pm1seBUUV7scCWbT31LsPMbEhwYFQxacwIlq97vt5lmJkNCQ6MKs6aNo7HntnM6g0v1bsUM7O6\nyzUwJM2StErSakkXl9n+fkkrJD0o6V5JJ2W2PZ6uXy6pM886K5k17UgAbnnw6Xp8vZnZkJJbYEgq\nAFcCZwHHA3MlHV+y21rgjIh4A/AFYEHJ9jMjYnpEdORVZzVHHNxGxzGvYclKB4aZWZ4tjBnA6ohY\nExHbgOuB2dkdIuLeiHguXVwKTMixnn0ya9qRPPLUizz+7Mv1LsXMrK7yDIzxwLrMcle6rpKPALdk\nlgO4Q9IySfNyqK8mZ71hHAC3uJVhZsPckBj0lnQmSWD8Y2b16RExnaRL6+OS3lLh2HmSOiV1dnd3\nD3ht4w9t56SJh3LLyqcG/NxmZgeSPANjPTAxszwhXbcbSScCVwOzI2Jj3/qIWJ/+uQG4iaSLaw8R\nsSAiOiKiY+zYsQNY/i7vmnYkK7peYN2mLbmc38zsQJBnYNwPTJU0WVILMAdYnN1B0tHAjcBfRsRj\nmfUHSRrV9xl4B7Ayx1qrOmta0i11q7ulzGwYyy0wIqIHuAC4DXgEuCEiHpJ0vqTz090uBUYDV5Xc\nPnsE8CtJvwV+Dfw0Im7Nq9a9OXr0CF4/7mB+/ugz9SrBzKzumvM8eUQsAZaUrJuf+XwecF6Z49YA\nJ5Wur6fTpozmu0ufYGvPDlqbC/Uux8xs0A2JQe8DwSmTD2NrTy+/XfdCvUsxM6sLB0aNZkw+DAmW\nrtm4953NzBqQA6NGh45o4bgjD+a+tQ4MMxueHBj9cOqxh7HsiefY1tNb71LMzAadA6MfTpk8mle3\n97Kiy688N7Phx4HRDzMmHwZ4HMPMhicHRj8cdlALxx05ivvWbqp3KWZmg86B0U+nTD6MzsefY/sO\nj2OY2fDiwOinU44dzSvbd7Ciy89jmNnwkuuT3o2obxzj1pVPcVBrgd5emHBYOwe3FetcmZlZvhwY\n/TRmZCvHHTmKb969lm/evRaAUa3NnD9zCh85fTJtRb82xMwakwNjH8z/wB/z0B9eRIIIuOk367ns\ntlV8d+kT/Mu5J/DOE46sd4lmZgNOEVHvGgZMR0dHdHZ27n3HHNy3ZiOf+6+HWd29mZ9eeDpTjxhV\nlzrMzPpD0rKI6KhlXw96D5BTjh3NNR+ZwcjWZi76wW/p8V1UZtZgHBgDaMzIVr4wexorul7gG3et\nqXc5ZmYDyoExwP7sxHGcfeI4Lr/jMR59+sV6l2NmNmAcGDn4/OxpHNJe5MJrf8Oml7fVuxwzswHh\nwMjBYQe18NW5J/Pkpi28/+r7eM6hYWYNwIGRk9OmjOGbH+zg992bef/V9/H8FoeGmR3YfFttzn65\nagPzrllGc0G0Nif53F4sMPGwERwzegTHHXkwZ584jsMPbqtzpWY2HPXnttpcA0PSLOArQAG4OiK+\nVLJd6fZ3AVuAv4qIB2o5tpyhGBiQPKNx84qnAJBg89Yenty4hSc2baH7pa00Cf7H1LG85+TxnPba\n0Rw+yuFhZoOjP4GR25PekgrAlcDbgS7gfkmLI+LhzG5nAVPTn1OArwOn1HjsAeOUY0dzyrGjy277\nffdmbnygixsfWM8nv78cgNcePpITJxxCSyFpkUjQ2lygtdjEiGIzo0e2MHZUK2NGtnJIe5GD25sZ\n1VqkKdPB2CSlP5DkspnZ/snz1SAzgNURsQZA0vXAbCD7S382cE0kzZylkg6VNA6YVMOxDWHK2JF8\n5p3H8am3v44H17/A0jUbWbpmI/eu3kiQtP56A7Zu38HWnl627sP0sM1NolhoormQ/FloEoU0TEr1\nhYuUhI4EIvlMDblTukveYeUoNIPXjGjhhvPfnPv35BkY44F1meUuklbE3vYZX+OxAEiaB8wDOPro\no/ev4joqNInpEw9l+sRDOf+MKRX3276jl00vb6P7pa10b97Ki69s58VXe3jp1e1kexcjgt6AHb1B\nT28v23cE23p60+Uo+yR6kLwbKwgI6I3Yua63hq7LPfbIeXgs8v4CswPEYL0t+4B/+WBELAAWQDKG\nUedyclcsNHHEwW0c4UFyMxtkeQbGemBiZnlCuq6WfYo1HGtmZoMoz+cw7gemSposqQWYAywu2Wcx\n8EElTgVeiIinajzWzMwGUW4tjIjokXQBcBvJrbELI+IhSeen2+cDS0huqV1Nclvth6sdm1etZma2\nd35wz8xsGPN8GGZmNuAcGGZmVhMHhpmZ1cSBYWZmNWmoQW9J3cAT+3j4GODZASznQOBrbnzD7XrB\n19xfx0TE2Fp2bKjA2B+SOmu9U6BR+Job33C7XvA158ldUmZmVhMHhpmZ1cSBscuCehdQB77mxjfc\nrhd8zbnxGIaZmdXELQwzM6uJA8PMzGoy7AND0ixJqyStlnRxvevJg6SJkn4h6WFJD0n6RLr+MEk/\nk/S79M/X1LvWgSapIOk3km5Olxv6mtNpjn8o6VFJj0h68zC45r9P/7teKek6SW2Nds2SFkraIGll\nZl3Fa5R0Sfo7bZWkdw5UHcM6MCQVgCuBs4DjgbmSjq9vVbnoAS6KiOOBU4GPp9d5MfDziJgK/Dxd\nbjSfAB7JLDf6NX8FuDUijgNOIrn2hr1mSeOBvwM6ImIayXQIc2i8a14EzCpZV/Ya0/9vzwFOSI+5\nKv1dt9+GdWAAM4DVEbEmIrYB1wOz61zTgIuIpyLigfTzSyS/RMaTXOt30t2+A7y7PhXmQ9IE4M+A\nqzOrG/aaJR0CvAX4FkBEbIuI52nga041A+2SmoERwB9osGuOiLuATSWrK13jbOD6iNgaEWtJ5hua\nMRB1DPfAGA+syyx3pesalqRJwMnAfcAR6QyHAE8DR9SprLxcDvwD0JtZ18jXPBnoBr6ddsNdLekg\nGviaI2I98GXgSeApklk7b6eBrzmj0jXm9nttuAfGsCJpJPAj4JMR8WJ2WyT3VzfMPdaSzgY2RMSy\nSvs02jWT/Ev7jcDXI+Jk4GVKumIa7ZrTfvvZJGF5FHCQpA9k92m0ay5nsK5xuAfGemBiZnlCuq7h\nSCqShMX3IuLGdPUzksal28cBG+pVXw7+BDhX0uMkXY1/Kum7NPY1dwFdEXFfuvxDkgBp5Gt+G7A2\nIrojYjtwI3AajX3NfSpdY26/14Z7YNwPTJU0WVILyUDR4jrXNOAkiaRf+5GI+I/MpsXAh9LPHwJ+\nMti15SUiLomICRExieR/1/+OiA/Q2Nf8NLBO0uvSVW8FHqaBr5mkK+pUSSPS/87fSjJG18jX3KfS\nNS4G5khqlTQZmAr8eiC+cNg/6S3pXSR93QVgYUR8sc4lDThJpwN3Aw+yqz//n0jGMW4AjiZ5Lfxf\nRETpwNoBT9JM4NMRcbak0TTwNUuaTjLI3wKsAT5M8g/DRr7mzwH/m+RuwN8A5wEjaaBrlnQdMJPk\nNebPAJ8FfkyFa5T0z8Bfk/ydfDIibhmQOoZ7YJiZWW2Ge5eUmZnVyIFhZmY1cWCYmVlNHBhmZlYT\nB4aZmdXEgWGWkb7t9W8zyzP73nSb43ceJemHFbb9UlLHPp73ryRdsX/Vme3iwDDb3aHA3+51ryrS\nl+DVLCL+EBHv3Z/vNBsMDgwbFiRNSueH+GY6d8LtktrL7PolYIqk5ZIuS9eNzMwx8b30ieLS8/9S\n0uWSOoFPSBor6UeS7k9//iTd74z03MvTFwSOSmtbmW5vl3R9WutNQHvmOzZnPr9X0qL08zmS7kvP\nd4ekRnzRng0B/fqXkNkBbiowNyI+KukG4H8B3y3Z52JgWkRMh51PiZ9MMrfAH4B7SN5T9asy52+J\niI70uGuB/4yIX0k6GrgNeD3waeDjEXFP+jLIV0vO8TFgS0S8XtKJwAM1XNevgFMjIiSdR/KG3otq\nOM6sXxwYNpysjYjl6edlwKQaj/t1RHQBSFqeHlcuML6f+fw24PhMY+TgNCDuAf5D0veAGyOiq6TB\n8hbgqwARsULSihrqmwB8P30BXQuwtsbrMusXd0nZcLI183kH0Kxk+tq+LqLzaz2uwn4vZz43kfyr\nf3r6Mz4iNkfEl0jeddQO3CPpuH7Un32PT1vm89eAKyLiDcDflGwzGzAODBvWImJd5pf6fOAlYNQA\nnPp24MK+hfSlgEiaEhEPRsS/k7wtuTQw7gLel+47DTgxs+0ZSa+X1AS8J7P+EHa9vvpDmOXEgWGW\nEREbSf5feEWqAAAAdUlEQVTlvzIz6L0v/g7okLRC0sNAX+vlk+m5VwDbgdK3iH6dZJD9EeDzJF1n\nfS4GbgbuJZldrs+/AD+QtAx4dj9qNqvKb6s1M7OauIVhZmY1cWCYmVlNHBhmZlYTB4aZmdXEgWFm\nZjVxYJiZWU0cGGZmVpP/D+g3Uo3L10rDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f53c281c668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(res_approximated[-1])\n",
    "plt.plot(res_approximated)\n",
    "plt.title('Residuals')\n",
    "plt.xlabel('n-th residual')\n",
    "plt.ylabel('Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it does not converge because it stuck in the isolated node 3 and 4 and moves around them (random walk process, a bit similar to markov chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.,  0.,  0.],\n",
       "       [ 1.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_modified = np.array([[0,0,1,0,0],[1,0,1,0,0],[0,1,0,0,0],[0,0,0,0,1],[0,0,0,0,0]],dtype=np.float32)\n",
    "G_modified = np.ndarray(shape=G_modified.shape, buffer = G_modified, dtype=np.float32)\n",
    "G_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A=pagerank_matrix(G_modified)\n",
    "x0 = np.random.rand(1,A.shape[0])\n",
    "x0 = np.ndarray(shape=x0.shape, buffer=x0, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_approximated_mod, l_approximated_mod, res_approximated_mod = power_method(A, x0, num_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJGno/wOdxI/niexNwAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f53c27c1080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(res_approximated_mod)\n",
    "plt.title('Residuals')\n",
    "plt.xlabel('n-th residual')\n",
    "plt.ylabel('Value')\n",
    "print(res_approximated_mod[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the power method with num_iter=100 for 10 different initial guesses and print/plot the resulting approximated eigenvectors. Why do they depend on the initial guess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5]\n",
      " [ 1. ]\n",
      " [ 1. ]\n",
      " [ 0. ]\n",
      " [ 0. ]] \n",
      "\n",
      "[[ 0.5]\n",
      " [ 1. ]\n",
      " [ 1. ]\n",
      " [ 0. ]\n",
      " [ 0. ]] \n",
      "\n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]] \n",
      "\n",
      "[[ 0.5]\n",
      " [ 1. ]\n",
      " [ 1. ]\n",
      " [ 0. ]\n",
      " [ 0. ]] \n",
      "\n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]] \n",
      "\n",
      "[[ 0.5]\n",
      " [ 1. ]\n",
      " [ 1. ]\n",
      " [ 0. ]\n",
      " [ 0. ]] \n",
      "\n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]] \n",
      "\n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]] \n",
      "\n",
      "[[ 0.5]\n",
      " [ 1. ]\n",
      " [ 1. ]\n",
      " [ 0. ]\n",
      " [ 0. ]] \n",
      "\n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(None)\n",
    "for _ in range(10):\n",
    "    x0 = np.random.rand(1,A.shape[0])\n",
    "    x0 = np.ndarray(shape=x0.shape, buffer=x0, dtype=np.float32)\n",
    "    x_approximated_ran, l_approximated_ran, res_approximated_ran = power_method(A, x0, num_iter=100)\n",
    "    print(x_approximated_ran,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between eigenvectors can be explained if we imagine this process as a random walk. Here, our random walker just stucks in one node and can't go further, so if our initial guess is randomly bad, we arrive at this state soon and have all zeros in eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Implement fast matrix-vector product for $A_d$ as a function ```pagerank_matvec(A, d, x)```, which takes a PageRank matrix $A$ (in sparse format, e.g., ```csr_matrix```), damping factor $d$ and a vector $x$ as an input and returns $A_dx$ as an output. Generate a random adjacency matrix of size $10000 \\times 10000$ with only 100 non-zero elements and compare ```pagerank_matvec``` performance with direct evaluation of $A_dx$."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# INPUT:  A - np.ndarray (2D), d - float (from 0.0 to 1.0), x - np.ndarray (1D, size of A.shape[0/1])\n",
    "# OUTPUT: y - np.ndarray (1D, size of x)\n",
    "def pagerank_matvec(A, d, x): # 2 pts\n",
    "    N = x.shape\n",
    "    A_ = scipy.sparse.csr_matrix(A)\n",
    "    B_ = scipy.sparse.csr_matrix(np.ones_like(A))\n",
    "    y = A_.dot(x)*d + (1-d)/ N * B_.dot(x)\n",
    "    #y = A.dot(x) * d + (1 - d) / N * np.dot(np.ones_like(A_), x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.random.seed(None)\n",
    "\n",
    "def get_rand_adj(N):\n",
    "    matrix = np.zeros((N,N))\n",
    "    for _ in range(100):\n",
    "        i = np.random.randint(0,N)\n",
    "        j = np.random.randint(0,N)\n",
    "        if not bool(matrix[i,j]):\n",
    "            matrix[i,j] = 1.\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Matv = get_rand_adj(10000)\n",
    "vec = np.random.rand(Matv.shape[0],1)\n",
    "vec = np.ndarray(shape=vec.shape, buffer=vec, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### DBLP: computer science bibliography\n",
    "\n",
    "Download the dataset from [here](https://goo.gl/oZVxEa), unzip it and put `dblp_authors.npz`  and `dblp_graph.npz` in the same folder with this notebook. Each value (author name) from `dblp_authors.npz` corresponds to the row/column of the matrix from `dblp_graph.npz`. Value at row `i` and column `j` of the matrix from `dblp_graph.npz` corresponds to the number of times author `i` cited papers of the author `j`. Let us now find the most significant scientists according to PageRank model over DBLP data.\n",
    "\n",
    "* Load the weighted adjacency matrix and the authors list into Python using ```load_dblp(...)``` function. Print its density (fraction of nonzero elements). Find top-10 most cited authors from the weighted adjacency matrix. Now, make all the weights of the adjacency matrix equal to 1 for simplicity (consider only existence of connection between authors, not its weight). Obtain the PageRank matrix $A$ from the adjacency matrix and verify that it is stochastic.\n",
    " \n",
    " \n",
    "* In order to provide ```pagerank_matvec``` to your ```power_method``` (without rewriting it) for fast calculation of $A_dx$, you can create a ```LinearOperator```: \n",
    "```python\n",
    "L = scipy.sparse.linalg.LinearOperator(A.shape, matvec=lambda x, A=A, d=d: pagerank_matvec(A, d, x))\n",
    "```\n",
    "Calling ```L@x``` or ```L.dot(x)``` will result in calculation of ```pagerank_matvec(A, d, x)``` and, thus, you can plug $L$ instead of the matrix $A$ in the ```power_method``` directly. **Note:** though in the previous subtask graph was very small (so you could disparage fast matvec implementation), here it is very large (but sparse), so that direct evaluation of $A_dx$ will require $\\sim 10^{12}$ matrix elements to store - good luck with that (^_<)≡☆\n",
    "\n",
    "\n",
    "* Run the power method starting from the vector of all ones and plot residuals $\\|A_dx_k - \\lambda_k x_k\\|_2$  as a function of $k$ for $d=0.85$.\n",
    "\n",
    "\n",
    "* Print names of the top-10 authors according to PageRank over DBLP when $d=0.85$.\n",
    "\n",
    "\n",
    "* (Bonus) Does it look suspicious? Why? Discuss what could cause such results."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from scipy.sparse import load_npz\n",
    "import numpy as np\n",
    "def load_dblp(path_auth, path_graph):\n",
    "    G = load_npz(path_graph).astype(float)\n",
    "    with np.load(path_auth) as data: authors = data['authors']\n",
    "    return G, authors\n",
    "G, authors = load_dblp('dblp_authors.npz', 'dblp_graph.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. QR algorithm (10 pts)\n",
    "\n",
    "* Implement QR-algorithm without shifting. Prototype of the function is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INPUT: \n",
    "# A_init - square matrix, \n",
    "# num_iter - number of iterations for QR algorithm\n",
    "# OUTPUT: \n",
    "# Ak - transformed matrix A_init given by QR algorithm, \n",
    "# convergence - numpy array of shape (num_iter, ), \n",
    "# where we store the maximal number from the Chebyshev norm \n",
    "# of triangular part of the Ak for every iteration\n",
    "\n",
    "def qr_algorithm(A_init, num_iter): # 3 pts\n",
    "\n",
    "    conv_hist,A_ = np.zeros(num_iter),A_init.copy()\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        Q,R = scipy.linalg.decomp_qr.qr(A_)\n",
    "        A_=np.dot(Q.T,np.dot(A_,Q))\n",
    "        conv_hist[i] = np.max(np.abs(A_[np.tril_indices(A_.shape[0], -1)]))\n",
    "        \n",
    "    return A_, conv_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symmetric case\n",
    "\n",
    "- Create **symmetric** tridiagonal $11 \\times 11$ matrix with elements $-1, 2, -1$ on sub-, main- and upper diagonal respectively without using loops. \n",
    "- Run $300$ iterations of the QR algorithm for this matrix. \n",
    "- Plot the output matrix with function ```plt.spy(Ak, precision=1e-7)```.\n",
    "- Plot convergence of QR-algorithm.\n",
    "\n",
    "<img src=https://pbs.twimg.com/media/Bq6t17OIMAALkiA.jpg width=30%/>\n",
    "*Photo comment*: professor Gilbert Strang (MIT) \"These are 121 cupcakes with my favorite -1, 2, -1 matrix. It was the day before Thanksgiving and two days before my birthday. A happy surprise.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonsymmetric case\n",
    "\n",
    "- Create **nonsymmetric** tridiagonal $11 \\times 11$ matrix with elements $5, 3, -2$ on sub-, main- and upper diagonal respectively without using loops. \n",
    "- Run $200$ iterations of the QR algorithm for this matrix. \n",
    "- Plot the result matrix with function ```plt.spy(Ak, precision=1e-7)```. Is this matrix lower triangular? How does this correspond to the claim about convergence of the QR algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sym_tri(N,elements=[-1,2,-1]):\n",
    "    x = np.zeros((11, 11), dtype=np.int)\n",
    "    i = j = np.arange(1,N+1,1)\n",
    "    ii,jj = np.meshgrid(i,j)\n",
    "    X = ii-jj\n",
    "    Func=np.vectorize(lambda x: elements[1]*(x==0)+elements[0]*(x==1)+elements[2]*(x==-1))\n",
    "    return Func(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=get_sym_tri(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_t, conv_t=qr_algorithm(A_init=x, num_iter=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f53c266a780>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nprrnl7Rgxi81ZfxSU8YvNWX8UlP/A4VT4M0Wf01lAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f53b956b3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.spy(a_t, precision=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=get_sym_tri(11,[-2,3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3, -2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 5,  3, -2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  5,  3, -2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  5,  3, -2,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  5,  3, -2,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  5,  3, -2,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  5,  3, -2,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  5,  3, -2,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  5,  3, -2,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  5,  3, -2],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  5,  3]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_t1, conv_t1=qr_algorithm(A_init=y, num_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f53c25f0ac8>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1JTxS039D+b771oZ0kIZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f53c26b5550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.spy(a_t1, precision=1e-7)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
